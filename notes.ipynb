{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbaea015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.empty(5,7,3)\n",
    "y=torch.empty(5,7,3)\n",
    "# same shapes are always broadcastable (i.e. the above rules always hold)\n",
    "\n",
    "x=torch.empty((0,))\n",
    "y=torch.empty(2,2)\n",
    "# x and y are not broadcastable, because x does not have at least 1 dimension\n",
    "\n",
    "# can line up trailing dimensions\n",
    "x=torch.empty(5,3,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "# x and y are broadcastable.\n",
    "# 1st trailing dimension: both have size 1\n",
    "# 2nd trailing dimension: y has size 1\n",
    "# 3rd trailing dimension: x size == y size\n",
    "# 4th trailing dimension: y dimension doesn't exist\n",
    "\n",
    "# but:\n",
    "x=torch.empty(5,2,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50174994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp(): tensor([ 2.7183,  7.3891, 20.0855])\n",
      "Sum of exp(): tensor(30.1929)\n",
      "softmax(): tensor([0.0900, 0.2447, 0.6652])\n",
      "Sum of softmax(): tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate exp() vs softmax() in PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example vector of logits\n",
    "logits = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# exp() applies the exponential function to each element\n",
    "exp_values = torch.exp(logits)\n",
    "print(\"exp():\", exp_values)\n",
    "print(\"Sum of exp():\", exp_values.sum())\n",
    "\n",
    "# softmax() applies exp() to each element and normalizes so the outputs sum to 1\n",
    "softmax_values = F.softmax(logits, dim=0)\n",
    "print(\"softmax():\", softmax_values)\n",
    "print(\"Sum of softmax():\", softmax_values.sum())\n",
    "\n",
    "# Notes:\n",
    "# - exp() just exponentiates each value, does not normalize, so the sum is not 1.\n",
    "# - softmax() exponentiates and normalizes, so the outputs are positive and sum to 1 (probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94174f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 2.5\n",
      "L2 regularization term: 0.07499999552965164\n",
      "Total loss with L2 regularization: 2.575000047683716\n"
     ]
    }
   ],
   "source": [
    "# Example of L2 regularization (weight decay) in loss calculation\n",
    "# L2 regularization helps prevent overfitting by discouraging large weights in the model.\n",
    "# It adds a penalty term to the loss, which is proportional to the average of the squared weights.\n",
    "# This encourages the model to keep weights small and find simpler solutions.\n",
    "\n",
    "import torch\n",
    "\n",
    "# Suppose W is a weight matrix\n",
    "W = torch.tensor([[1.0, -2.0], [3.0, -4.0]], requires_grad=True)\n",
    "\n",
    "# Example loss (e.g., from your model)\n",
    "loss = torch.tensor(2.5)\n",
    "\n",
    "# L2 regularization term\n",
    "l2_reg = 0.01 * (W ** 2).mean()\n",
    "\n",
    "# Total loss with L2 regularization\n",
    "loss_total = loss + l2_reg\n",
    "\n",
    "print(\"Original loss:\", loss.item())\n",
    "print(\"L2 regularization term:\", l2_reg.item())\n",
    "print(\"Total loss with L2 regularization:\", loss_total.item())\n",
    "\n",
    "# In practice, this helps the model generalize better and reduces the risk of memorizing the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C = torch.randn((32, 3))\n",
    "X = torch.randn((27, 2))\n",
    "\n",
    "X.shape  # (N, 3) - N samples, each with 3 character indices\n",
    "C.shape  # (27, 2) - 27 characters, each with 2D embedding\n",
    "\n",
    "emb = C[X]\n",
    "emb.shape  # (N, 3, 2)\n",
    "\n",
    "# Example: X has shape (32, 3)\n",
    "X[0] = [0, 0, 5]  # first context\n",
    "X[1] = [0, 5, 13] # second context\n",
    "# ... 30 more rows\n",
    "\n",
    "# C[X] returns:\n",
    "# Row 0: [C[0], C[0], C[5]]    → shape (3, 2)\n",
    "# Row 1: [C[0], C[5], C[13]]   → shape (3, 2)\n",
    "# ...\n",
    "# Final shape: (32, 3, 2)\n",
    "\n",
    "# This efficiently converts all your input sequences from character indices to their learned vector representations in one operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14bf23e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4713,  0.7868],\n",
       "        [-0.3284, -0.4330],\n",
       "        [ 1.3729,  2.9334]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "\n",
    "C[torch.tensor([5,6,7])]\n",
    "\n",
    "# Equivalent to:\n",
    "torch.stack([C[5], C[6], C[7]])\n",
    "\n",
    "# Returns something like:\n",
    "# tensor([[ 0.234, -1.123],   # embedding for character at index 5\n",
    "#         [-0.456,  0.789],   # embedding for character at index 6  \n",
    "#         [ 1.012, -0.334]])  # embedding for character at index 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb671b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b5e799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20d1d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohak\\AppData\\Local\\Temp\\ipykernel_45764\\214256462.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  a.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
